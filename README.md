### 1 ПРОХОД  
20 эпох
nn.Dropout(0.5)
triplet_loss = TripletLoss(margin=0.5) 
optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)

1 Эпоха 1, Тренировочные потери: 0.4949810561783817, Валидационные потери: 0.48952222723479666
 2 Эпоха 2, Тренировочные потери: 0.46600137030312777, Валидационные потери: 0.4563899987334505
… 
20 Эпоха 20, Тренировочные потери: 0.00417166882698689, Валидационные потери: 0.34425927499018677
21 Средние потери на тестовом наборе: 0.34425927499018677
22 Точность модели: 70.09%

#### Анализ: Обучение прошло успешно, но модель всё ещё недообучена, так как точность не достигла приемлемого уровня.
![image](https://github.com/user-attachments/assets/ced79683-aa63-47c0-8680-7f5e0b11a7ee)




### 2 ПРОХОД  
20 эпох 
nn.Dropout(0.5)
triplet_loss = TripletLoss(margin=1.0) 
optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-5)

1 Найдено файлов: 44242
 2 Пример файла: C:\Users\Андрей гл\OBUCHENIE\3 СЕМЕСТР\Нейросетевые технологии в задачах синтетических медиа\Практическая 3\VCTK-Corpus\wav48\p225\p225_001.wav
 3 Training set: 30969 samples
 4 Validation set: 6636 samples
 5 Test set: 6637 samples
 6 Эпоха 1, Тренировочные потери: 0.9809219972802958, Валидационные потери: 0.9110768399107347
24 Эпоха 19, Тренировочные потери: 0.01039499514693514, Валидационные потери: 0.9105880250624561
25 Эпоха 20, Тренировочные потери: 0.014517146950468011, Валидационные потери: 0.8980429608887489
26 Средние потери на тестовом наборе: 0.8980429608887489
27 Точность модели: 57.80%

#### Анализ: Увеличение параметра margin до 1.0 ухудшило результаты, это может указывать на чрезмерное разделение классов. Стоит уменьшить margin.
![image](https://github.com/user-attachments/assets/d3fd9c6a-d706-4dc8-b3cd-84c6cb327dfd)


### 3 ПРОХОД  
20 эпох
nn.Dropout(0.5)
triplet_loss = TripletLoss(margin=0.5) 
optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)

1 Найдено файлов: 44242
 2 Пример файла: C:\Users\Андрей гл\OBUCHENIE\3 СЕМЕСТР\Нейросетевые технологии в задачах синтетических медиа\Практическая 3\VCTK-Corpus\wav48\p225\p225_001.wav
 3 Training set: 30969 samples
 4 Validation set: 6636 samples
 5 Test set: 6637 samples
 6 Эпоха 1, Тренировочные потери: 0.495190919757983, Валидационные потери: 0.49981460035394093
…
24 Эпоха 19, Тренировочные потери: 0.4462939041470169, Валидационные потери: 0.497727004322437
25 Эпоха 20, Тренировочные потери: 0.43054349794300323, Валидационные потери: 0.49739285499677743
26 Средние потери на тестовом наборе: 0.49739285499677743
27 Точность модели: 53.94%
#### Анализ: Увеличение скорости обучения (lr=0.001) не привело к улучшению результатов, что указывает на переобучение модели.

![image](https://github.com/user-attachments/assets/df7415bb-14d5-446f-955e-9d50fa5456fa)



### 4 ПРОХОД  
20 эпох
nn.Dropout(0.5)
triplet_loss = TripletLoss(margin=0.5) 
optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)
1 Найдено файлов: 44242
 2 Пример файла: C:\Users\Андрей гл\OBUCHENIE\3 СЕМЕСТР\Нейросетевые технологии в задачах синтетических медиа\Практическая 3\VCTK-Corpus\wav48\p225\p225_001.wav
 3 Training set: 30969 samples
 4 Validation set: 6636 samples
 5 Test set: 6637 samples
 6 Эпоха 1, Тренировочные потери: 0.49193179432405243, Валидационные потери: 0.4727928264425435
….
24 Эпоха 19, Тренировочные потери: 0.0030675954774979056, Валидационные потери: 0.1890948116232496
25 Эпоха 20, Тренировочные потери: 0.00547461214415524, Валидационные потери: 0.21195449118220477
26 Средние потери на тестовом наборе: 0.21195449118220477
27 Точность модели: 82.57%

#### Анализ: Параметры weight_decay и lr=0.0001 оказались более удачными. Уменьшение weight_decay помогло снизить потери и повысить точность.

 ![image](https://github.com/user-attachments/assets/43cc3a7e-2b7b-4039-b414-e98959eca678)

### 5 ПРОХОД   НАШЕЛ ПРОБЛЕМУ ТЕСТОВЫЕ ДАННЫЕ ТЕРЯЛИСЬ. БУДУ ИСПОЛЬЗОВАТЬ ИХ В КАЧЕТСВЕ ВАЛИДАЦИОННЫХ
50 эпох
nn.Dropout(0.5)
triplet_loss = TripletLoss(margin=0.5) 
optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)

6 Эпоха 1, Тренировочные потери: 0.5138219891338174, Валидационные потери: 0.49841289214037976
48 Эпоха 43, Тренировочные потери: 0.003889980447401694, Валидационные потери: 0.34984564114054406
49 Эпоха 44, Тренировочные потери: 0.004608077193618915, Валидационные потери: 0.3042792259006325
50 Эпоха 45, Тренировочные потери: 0.0007289685240579307, Валидационные потери: 0.29931810602135617
51 Эпоха 46, Тренировочные потери: 0.0017169668040144335, Валидационные потери: 0.2895002024983047
52 Эпоха 47, Тренировочные потери: 0.0020547960876324857, Валидационные потери: 0.28856215039524463
53 Эпоха 48, Тренировочные потери: 0.003221975225921071, Валидационные потери: 0.2812678223356194
54 Эпоха 49, Тренировочные потери: 0.0022214884058051154, Валидационные потери: 0.3036908404542765
55 Эпоха 50, Тренировочные потери: 0.002824317424669178, Валидационные потери: 0.3049688399384875
56 Средние потери на тестовом наборе: 0.33173248910029
57 Точность модели: 72.48

#### Анализ: Более длительное обучение (50 эпох) улучшило результаты, но точность по-прежнему ниже ожиданий.

![image](https://github.com/user-attachments/assets/d4e34f74-3063-4741-89e4-efc964048a6c)


### 6 ПРОХОД  
50 эпох
nn.Dropout(0.5)
triplet_loss = TripletLoss(margin=0.5) 
optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)
6 Эпоха 1, Тренировочные потери: 0.4893746157304956, Валидационные потери: 0.49564873690998884
…
54 Эпоха 49, Тренировочные потери: 0.019151040610917117, Валидационные потери: 0.523874349659736
55 Эпоха 50, Тренировочные потери: 0.018200316122912487, Валидационные потери: 0.5231163149579949
56 Средние потери на тестовом наборе: 0.5235174238134962
57 Точность модели: 55.41%
#### Анализ: Увеличение скорости обучения вновь привело к переобучению, с более высокими потерями и более низкой точностью.

![image](https://github.com/user-attachments/assets/163b23fe-a317-4f71-928e-f48f279e511f)


7 ПРОХОД  
100 эпох
nn.Dropout(0.5)
triplet_loss = TripletLoss(margin=0.5) 
optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=1e-4)
#### Анализ:
Переобучение: Модель отлично справляется с тренировкой (почти нулевые потери), но на валидации наблюдаются колебания, что говорит о переобучении.
Улучшение обобщающей способности: Можно увеличить Dropout или применить другие методы регуляризации для повышения качества на валидационных и тестовых данных.
Подбор параметров: Рекомендуется уменьшить margin в Triplet Loss до 0.5, чтобы снизить чувствительность к разнице между «позитивными» и «негативными» примерами.


### Выводы:

    Более длительное обучение (50-100 эпох) стабильно улучшает результаты.
    Оптимальные гиперпараметры включают: Dropout=0.5, TripletLoss с margin=0.5, lr=0.0001, weight_decay=1e-4.
    Повышение скорости обучения до lr=0.001 приводит к ухудшению результатов, особенно на валидационном наборе, что может свидетельствовать о переобучении.

![image](https://github.com/user-attachments/assets/ac79a04a-d026-4d36-9c2b-899a5e35c753)

    
# -
